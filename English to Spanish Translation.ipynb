{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/spa.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the text file of Spanish-English pairs:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corran!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corra!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corred!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Corred.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input    target                                           comments\n",
       "0   Go.       Ve.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1   Go.     Vete.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2   Go.     Vaya.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3   Go.   Váyase.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "4   Hi.     Hola.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "5  Run!   ¡Corre!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "6  Run!  ¡Corran!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "7  Run!   ¡Corra!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "8  Run!  ¡Corred!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "9  Run.   Corred.  CC-BY 2.0 (France) Attribution: tatoeba.org #4..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=pd.read_table(\"/kaggle/input/spa.txt\",names=['input','target','comments'])\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124320</th>\n",
       "      <td>There are four main causes of alcohol-related ...</td>\n",
       "      <td>Hay cuatro causas principales de muertes relac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124321</th>\n",
       "      <td>There are mothers and fathers who will lie awa...</td>\n",
       "      <td>Hay madres y padres que se quedan despiertos d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124322</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Una huella de carbono es la cantidad de contam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124323</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Como suele haber varias páginas web sobre cual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124324</th>\n",
       "      <td>If you want to sound like a native speaker, yo...</td>\n",
       "      <td>Si quieres sonar como un hablante nativo, debe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input  \\\n",
       "0                                                     Go.   \n",
       "1                                                     Go.   \n",
       "2                                                     Go.   \n",
       "3                                                     Go.   \n",
       "4                                                     Hi.   \n",
       "...                                                   ...   \n",
       "124320  There are four main causes of alcohol-related ...   \n",
       "124321  There are mothers and fathers who will lie awa...   \n",
       "124322  A carbon footprint is the amount of carbon dio...   \n",
       "124323  Since there are usually multiple websites on a...   \n",
       "124324  If you want to sound like a native speaker, yo...   \n",
       "\n",
       "                                                   target  \n",
       "0                                                     Ve.  \n",
       "1                                                   Vete.  \n",
       "2                                                   Vaya.  \n",
       "3                                                 Váyase.  \n",
       "4                                                   Hola.  \n",
       "...                                                   ...  \n",
       "124320  Hay cuatro causas principales de muertes relac...  \n",
       "124321  Hay madres y padres que se quedan despiertos d...  \n",
       "124322  Una huella de carbono es la cantidad de contam...  \n",
       "124323  Como suele haber varias páginas web sobre cual...  \n",
       "124324  Si quieres sonar como un hablante nativo, debe...  \n",
       "\n",
       "[124325 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=lines[['input','target']]\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have separated the English and Spanish Sentences.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49381</th>\n",
       "      <td>What position do you hold?</td>\n",
       "      <td>¿Qué postura sostienes?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95846</th>\n",
       "      <td>The trains in Serbia are terribly slow.</td>\n",
       "      <td>Los trenes en Serbia van excesivamente lentos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50341</th>\n",
       "      <td>He became a nice young man.</td>\n",
       "      <td>Él se convirtió en un excelente joven.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109643</th>\n",
       "      <td>Why in the world would I want to be a teacher?</td>\n",
       "      <td>¿Por qué diablos yo querría ser profesora?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17575</th>\n",
       "      <td>I talk in my sleep.</td>\n",
       "      <td>Hablo mientras duermo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 input  \\\n",
       "49381                       What position do you hold?   \n",
       "95846          The trains in Serbia are terribly slow.   \n",
       "50341                      He became a nice young man.   \n",
       "109643  Why in the world would I want to be a teacher?   \n",
       "17575                              I talk in my sleep.   \n",
       "\n",
       "                                                target  \n",
       "49381                          ¿Qué postura sostienes?  \n",
       "95846   Los trenes en Serbia van excesivamente lentos.  \n",
       "50341           Él se convirtió en un excelente joven.  \n",
       "109643      ¿Por qué diablos yo querría ser profesora?  \n",
       "17575                           Hablo mientras duermo.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=lines.sample(70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now preprocessing the statements:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    num_digits= str.maketrans('','', digits)\n",
    "    \n",
    "    sentence= sentence.lower()\n",
    "    sentence= re.sub(\" +\", \" \", sentence)\n",
    "    sentence= re.sub(\"'\", '', sentence)\n",
    "    sentence= sentence.translate(num_digits)\n",
    "    sentence= re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.rstrip().strip()\n",
    "    sentence=  'start_ ' + sentence + ' _end'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['input']=lines['input'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30735                   start_ the suspect confessed . _end\n",
       "56014             start_ is the beam solid or hollow ? _end\n",
       "44856                start_ we ought to be back soon . _end\n",
       "113311    start_ venice ,  italy is one of the wonders o...\n",
       "100949    start_ what do you like most about working her...\n",
       "                                ...                        \n",
       "118517    start_ tom couldnt find a job in boston ,  so ...\n",
       "84562       start_ the problem is youre not canadian . _end\n",
       "34034                   start_ ill tell you a secret . _end\n",
       "42877                 start_ im going to take a bath . _end\n",
       "329                                start_ hi ,  guys . _end\n",
       "Name: input, Length: 70000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['target']=lines['target'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30735                   start_ el sospechoso confesó . _end\n",
       "56014           start_ ¿ está la viga sólida o hueca ? _end\n",
       "44856                start_ deberíamos volver pronto . _end\n",
       "113311    start_ venecia en italia es una de las maravil...\n",
       "100949    start_ ¿ qué es lo que más te gusta de trabaja...\n",
       "                                ...                        \n",
       "118517    start_ tom no pudo encontrar trabajo en boston...\n",
       "84562     start_ el problema es que tú no eres canadiens...\n",
       "34034                   start_ te contaré un secreto . _end\n",
       "42877                           start_ voy a bañarme . _end\n",
       "329                     start_ ¿ qué pasa ,  troncos ? _end\n",
       "Name: target, Length: 70000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = lines.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['start_ the suspect confessed . _end',\n",
       "        'start_ el sospechoso confesó . _end'],\n",
       "       ['start_ is the beam solid or hollow ? _end',\n",
       "        'start_ ¿ está la viga sólida o hueca ? _end'],\n",
       "       ['start_ we ought to be back soon . _end',\n",
       "        'start_ deberíamos volver pronto . _end'],\n",
       "       ...,\n",
       "       ['start_ ill tell you a secret . _end',\n",
       "        'start_ te contaré un secreto . _end'],\n",
       "       ['start_ im going to take a bath . _end',\n",
       "        'start_ voy a bañarme . _end'],\n",
       "       ['start_ hi ,  guys . _end',\n",
       "        'start_ ¿ qué pasa ,  troncos ? _end']], dtype='<U279')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = np.array(rows)\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Made English Spanish Pairs.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "english=[]\n",
    "for i in lines['input']:\n",
    "    english.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish=[]\n",
    "for i in lines['target']:\n",
    "    spanish.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_ the suspect confessed . _end'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the input and target tokens:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "input_sentence_tokenizer.fit_on_texts(english)\n",
    "input_array = input_sentence_tokenizer.texts_to_sequences(english)\n",
    "input_array= tf.keras.preprocessing.sequence.pad_sequences(input_array,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    4, 2115, ...,    0,    0,    0],\n",
       "       [   1,   11,    4, ...,    0,    0,    0],\n",
       "       [   1,   31,  990, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   84,   93, ...,    0,    0,    0],\n",
       "       [   1,   35,   72, ...,    0,    0,    0],\n",
       "       [   1, 2227,   17, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_sentence_tokenizer.fit_on_texts(spanish)\n",
    "target_array = target_sentence_tokenizer.texts_to_sequences(spanish)\n",
    "target_array= tf.keras.preprocessing.sequence.pad_sequences(target_array,padding='post',maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  2, ...,  0,  0,  0],\n",
       "       [ 1, 10,  2, ...,  0,  0,  0],\n",
       "       [ 1,  3,  2, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 1,  7,  3, ...,  0,  0,  0],\n",
       "       [ 1,  7,  3, ...,  0,  0,  0],\n",
       "       [ 1, 18,  3, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(target_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "max_target_length= max(len(t) for t in  target_array)\n",
    "print(max_target_length)\n",
    "max_source_length= max(len(t) for t in  input_array)\n",
    "print(max_source_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding the sentences to a certain length::-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "input_sentence_tokenizer.fit_on_texts(english)\n",
    "input_array = input_sentence_tokenizer.texts_to_sequences(english)\n",
    "input_array= tf.keras.preprocessing.sequence.pad_sequences(input_array,padding='post',maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_sentence_tokenizer.fit_on_texts(spanish)\n",
    "target_array = target_sentence_tokenizer.texts_to_sequences(spanish)\n",
    "target_array= tf.keras.preprocessing.sequence.pad_sequences(target_array,padding='post',maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7fed21ebda50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7fed21cc10d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentence_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000 56000 14000 14000\n"
     ]
    }
   ],
   "source": [
    "input_train, input_val, target_train, target_val = train_test_split(input_array, target_array, test_size=0.2)\n",
    "\n",
    "print(len(input_train), len(target_train), len(input_val), len(target_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80-20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11128\n"
     ]
    }
   ],
   "source": [
    "print(len(input_sentence_tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20980\n"
     ]
    }
   ],
   "source": [
    "print(len(target_sentence_tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is big, we want to create the dataset in memory to be efficient. We will use tf.data.Dataset.from_tensor_slices() method to get slices of the array in the form of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(input_train)\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(input_train)//batch_size\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_input_size = len(input_sentence_tokenizer.word_index)+1\n",
    "vocab_target_size = len(target_sentence_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "# print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Architecture:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_size,units,batchsize):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batchsize=batchsize\n",
    "        self.units=units\n",
    "        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru=tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')# entire sequence of outputs will be returned from all the units.\n",
    "        #To return the internal state of GRU, we set the return_state to True\n",
    "        # this returns 3 parameters in LSTM with return states and sequences true but in GRU it returns only 2 parameters..\n",
    "    \n",
    "    def call(self,y,hidden):\n",
    "        y=self.embedding(y)\n",
    "        output,state=self.gru(y,initial_state=hidden)\n",
    "        return output,state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batchsize, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 20]), TensorShape([64, 30]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 20, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_input_size, embedding_dim, units,batch_size )\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Bahdanau Attention Layer:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention layer consists:-\n",
    "\n",
    "Alignment Score\n",
    "\n",
    "Attention weights\n",
    "\n",
    "Context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention,self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    # The encoder hiiden states are taken as input to the attention layer which are of shape (batch size, units)\n",
    "    # and the encoder output of each timestep is of shape (batch size, sequence length, units).\n",
    "    # so for adding we have to expand dimensions\n",
    "    def call(self,encoder_out,encoder_hid):\n",
    "        \n",
    "       hidden1=tf.expand_dims(encoder_hid,1)\n",
    "        \n",
    "        \n",
    "       # score shape == (batch_size, max_length, 1)\n",
    "       # we get 1 at the last axis because we are applying score to self.V\n",
    "       # the shape of the array before applying self.V is (batch_size, max_length, units)\n",
    "       score = self.V(tf.nn.tanh(\n",
    "          self.W1(encoder_out) + self.W2(hidden1)))\n",
    "\n",
    "       # attention_weights shape == (batch_size, max_length, 1)\n",
    "       attention_weights = tf.nn.softmax(score, axis=1) ## the alignment scores for each encoder hidden state\n",
    "        #are combined and represented in a single vector and subsequently softmaxed\n",
    "\n",
    "       # context_vector shape after sum == (batch_size, hidden_size)\n",
    "       context_vector = attention_weights * encoder_out ## attention weights multiplied with the encoder output states are used to calculate the context vactor\n",
    "        \n",
    "       context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "       return context_vector, attention_weights ## returning the context vector and the attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = Attention(10)# 10 for units of attention\n",
    "attention_result, attention_weights = attention_layer(sample_output,sample_hidden)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The context vector should be of the shape of (batch size, units) as it be combined with the decoder previous embeddings.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Class:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_size,units,batchsize):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batchsize=batchsize\n",
    "        self.units=units\n",
    "        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru=tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')# entire sequence of outputs will be returned from all the units.\n",
    "        #To return the internal state of GRU, we set the return_state to True\n",
    "        \n",
    "        #fully connected layer for the decoder outputs\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # attention layer\n",
    "        self.attention=Attention(self.units)\n",
    "        \n",
    "    def call(self, x, enc_output,hidden):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(enc_output,hidden)\n",
    "#         print(context_vector.shape)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "#         print(x.shape)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)## context vaector is added with the previous decoder hidden state.\n",
    "       \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 20980)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_target_size, embedding_dim, units, batch_size)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                                      sample_output, sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Optimiser and Loss Function:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sentence_tokenizer.word_index['start_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, target, encoder_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    encoder_output, encoder_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoder_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * batch_size, 1)\n",
    "\n",
    "    # Teacher forcing \n",
    "    for t in range(1, target.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, decoder_hidden, _ = decoder(decoder_input, encoder_output, decoder_hidden)\n",
    "\n",
    "      loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(target[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:- 1 BATCH:- 0 LOSS:- 2.6692914962768555\n",
      "EPOCH:- 1 BATCH:- 100 LOSS:- 1.554011583328247\n",
      "EPOCH:- 1 BATCH:- 200 LOSS:- 1.5967878103256226\n",
      "EPOCH:- 1 BATCH:- 300 LOSS:- 1.7050445079803467\n",
      "EPOCH:- 1 BATCH:- 400 LOSS:- 1.5926105976104736\n",
      "EPOCH:- 1 BATCH:- 500 LOSS:- 1.5504059791564941\n",
      "EPOCH:- 1 BATCH:- 600 LOSS:- 1.6022757291793823\n",
      "EPOCH:- 1 BATCH:- 700 LOSS:- 1.656678557395935\n",
      "EPOCH:- 1 BATCH:- 800 LOSS:- 1.3925235271453857\n",
      "EPOCH:- 1 Loss:- 1.57393\n",
      "Time taken for this Epoch 466.7751874923706 sec\n",
      "\n",
      "EPOCH:- 2 BATCH:- 0 LOSS:- 1.414547324180603\n",
      "EPOCH:- 2 BATCH:- 100 LOSS:- 1.3436685800552368\n",
      "EPOCH:- 2 BATCH:- 200 LOSS:- 1.378778338432312\n",
      "EPOCH:- 2 BATCH:- 300 LOSS:- 1.3461782932281494\n",
      "EPOCH:- 2 BATCH:- 400 LOSS:- 1.3486477136611938\n",
      "EPOCH:- 2 BATCH:- 500 LOSS:- 1.2581250667572021\n",
      "EPOCH:- 2 BATCH:- 600 LOSS:- 1.426490068435669\n",
      "EPOCH:- 2 BATCH:- 700 LOSS:- 1.2014590501785278\n",
      "EPOCH:- 2 BATCH:- 800 LOSS:- 1.3172775506973267\n",
      "EPOCH:- 2 Loss:- 1.31184\n",
      "Time taken for this Epoch 464.2180075645447 sec\n",
      "\n",
      "EPOCH:- 3 BATCH:- 0 LOSS:- 1.2088454961776733\n",
      "EPOCH:- 3 BATCH:- 100 LOSS:- 1.1700340509414673\n",
      "EPOCH:- 3 BATCH:- 200 LOSS:- 1.1149362325668335\n",
      "EPOCH:- 3 BATCH:- 300 LOSS:- 1.1964666843414307\n",
      "EPOCH:- 3 BATCH:- 400 LOSS:- 1.1171270608901978\n",
      "EPOCH:- 3 BATCH:- 500 LOSS:- 1.1519947052001953\n",
      "EPOCH:- 3 BATCH:- 600 LOSS:- 1.0099966526031494\n",
      "EPOCH:- 3 BATCH:- 700 LOSS:- 1.122705101966858\n",
      "EPOCH:- 3 BATCH:- 800 LOSS:- 1.0638222694396973\n",
      "EPOCH:- 3 Loss:- 1.09997\n",
      "Time taken for this Epoch 461.39597392082214 sec\n",
      "\n",
      "EPOCH:- 4 BATCH:- 0 LOSS:- 0.9654774069786072\n",
      "EPOCH:- 4 BATCH:- 100 LOSS:- 1.0649954080581665\n",
      "EPOCH:- 4 BATCH:- 200 LOSS:- 0.9339174032211304\n",
      "EPOCH:- 4 BATCH:- 300 LOSS:- 0.9892944097518921\n",
      "EPOCH:- 4 BATCH:- 400 LOSS:- 0.8083403706550598\n",
      "EPOCH:- 4 BATCH:- 500 LOSS:- 0.889366090297699\n",
      "EPOCH:- 4 BATCH:- 600 LOSS:- 0.9105899930000305\n",
      "EPOCH:- 4 BATCH:- 700 LOSS:- 1.0936789512634277\n",
      "EPOCH:- 4 BATCH:- 800 LOSS:- 1.1036115884780884\n",
      "EPOCH:- 4 Loss:- 0.94269\n",
      "Time taken for this Epoch 459.4465956687927 sec\n",
      "\n",
      "EPOCH:- 5 BATCH:- 0 LOSS:- 0.7743297219276428\n",
      "EPOCH:- 5 BATCH:- 100 LOSS:- 0.8325257897377014\n",
      "EPOCH:- 5 BATCH:- 200 LOSS:- 0.8480527997016907\n",
      "EPOCH:- 5 BATCH:- 300 LOSS:- 0.9006614089012146\n",
      "EPOCH:- 5 BATCH:- 400 LOSS:- 0.9128695726394653\n",
      "EPOCH:- 5 BATCH:- 500 LOSS:- 0.8036433458328247\n",
      "EPOCH:- 5 BATCH:- 600 LOSS:- 0.913811445236206\n",
      "EPOCH:- 5 BATCH:- 700 LOSS:- 0.8646337389945984\n",
      "EPOCH:- 5 BATCH:- 800 LOSS:- 0.8032195568084717\n",
      "EPOCH:- 5 Loss:- 0.81922\n",
      "Time taken for this Epoch 457.63557147979736 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    if batch % 100 == 0:\n",
    "      print('EPOCH:- {} BATCH:- {} LOSS:- {}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
    "   \n",
    "\n",
    "  print('EPOCH:- {} Loss:- {:.5f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for this Epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_target_length, max_source_length))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [input_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_source_length,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
    "\n",
    "  for t in range(max_target_length):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         enc_out,\n",
    "                                                         dec_hidden\n",
    "                                                         )\n",
    "\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "    if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
    "      return result, sentence\n",
    "\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "  \n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ hello . _end\n",
      "Predicted translation: hola . _end \n"
     ]
    }
   ],
   "source": [
    "translate('Hello.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ hello everyone . _end\n",
      "Predicted translation: hola todos . _end \n"
     ]
    }
   ],
   "source": [
    "translate('Hello everyone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ how are you ? _end\n",
      "Predicted translation: ¿ cómo estás _end \n"
     ]
    }
   ],
   "source": [
    "translate('How are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ i am working from home . _end\n",
      "Predicted translation: estoy trabajando . _end \n"
     ]
    }
   ],
   "source": [
    "translate('I am working from home.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ learning spanish . _end\n",
      "Predicted translation: aprender un _end \n"
     ]
    }
   ],
   "source": [
    "translate('Learning Spanish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ lets hope for the best . _end\n",
      "Predicted translation: esperemos por . _end \n"
     ]
    }
   ],
   "source": [
    "translate('Lets Hope for the best.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
